
import re
import pathlib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Optional 
from langchain_openai import ChatOpenAI
from langchain.tools import Tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from index_documents import get_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import Document

# --- –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ì–û –ü–û–õ–£–ß–ï–ù–ò–Ø –ü–†–û–î–£–ö–¢–û–í ---
def get_known_products(data_root_path: str = "data") -> dict:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é `data` –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤."""
    root = pathlib.Path(data_root_path)
    products = {}
    if not root.is_dir():
        return products
    for product_path in root.iterdir():
        if product_path.is_dir():
            product_name = product_path.name
            products[product_name.lower()] = product_name
    return products

# --- –ù–ê–°–¢–†–û–ô–ö–ê –û–°–ù–û–í–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í ---
llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
retriever = get_retriever(use_parent_document=True)
embeddings_model = retriever.vectorstore._embedding_function

# --- –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–û–î–£–ö–¢–û–í ---
auto_products = get_known_products()
CANONICAL_PRODUCT_NAMES = list(auto_products.values())
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–¥—É–∫—Ç—ã –∏–∑ –ø–∞–ø–æ–∫: {CANONICAL_PRODUCT_NAMES}")

# --- –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê –ü–†–û–î–£–ö–¢–ê ---
def _find_target_product_semantically(
    query: str, 
    product_names: list[str], 
    threshold=0.6
) -> Optional[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–π –ø—Ä–æ–¥—É–∫—Ç –∫ –∑–∞–ø—Ä–æ—Å—É, –∏—Å–ø–æ–ª—å–∑—É—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
    if not product_names:
        return None

    query_embedding = embeddings_model.embed_query(query)
    product_embeddings = embeddings_model.embed_documents(product_names)

    similarities = cosine_similarity([query_embedding], product_embeddings)[0]

    max_similarity_index = np.argmax(similarities)
    max_similarity = similarities[max_similarity_index]

    if max_similarity > threshold:
        return product_names[max_similarity_index]
    
    return None

# --- –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê –í –ë–ê–ó–ï –ó–ù–ê–ù–ò–ô ---
def _search_knowledge_base(query: str, k: int = 7) -> str:
    target_product = _find_target_product_semantically(query, CANONICAL_PRODUCT_NAMES)
    
    if target_product:
        print(f"‚ÑπÔ∏è  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø—Ä–æ–¥—É–∫—Ç: '{target_product}'")

    initial_results = retriever.get_relevant_documents(query, k=20)
    
    if not initial_results:
        return "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    if target_product:
        filtered_results = [
            doc for doc in initial_results if doc.metadata.get('product') == target_product
        ]
        if not filtered_results:
            return f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ '{target_product}'."
        results = filtered_results[:k]
    else:
        results = initial_results[:k]

    if not results:
        return "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    blob_parts = []
    for i, doc in enumerate(results):
        content = doc.page_content
        product = doc.metadata.get('product', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç')
        title = doc.metadata.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
        source_path = doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
        
        source_header = (
            f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: "
            f"Product='{product}', "
            f"Title='{title}', "
            f"SourceFile='{source_path}'"
        )
        blob_parts.append(f"===== {source_header} =====\n{content}\n")
        
    return "\n".join(blob_parts)

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≥–µ–Ω—Ç–∞ ---
retriever_tool = Tool(
    name="retriever",
    func=_search_knowledge_base,
    description="–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."
)
TOOLS = [retriever_tool]


SYSTEM_MSG = (
    "–í—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ—á–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n\n"
    "**–°–ê–ú–û–ï –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û:**\n"
    "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown: `![...](...)`. "
    "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –¥–æ –∏ –ø–æ—Å–ª–µ –Ω–µ–≥–æ, —è–≤–ª—è—é—Ç—Å—è **–µ–¥–∏–Ω—ã–º –Ω–µ–¥–µ–ª–∏–º—ã–º –±–ª–æ–∫–æ–º**. "
    "**–ó–ê–ü–†–ï–©–ï–ù–û** –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å, —Å–æ–∫—Ä–∞—â–∞—Ç—å –∏–ª–∏ –∏–∑–º–µ–Ω—è—Ç—å —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–∫—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –¢—ã –¥–æ–ª–∂–µ–Ω —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫ (—Ç–µ–∫—Å—Ç-–∫–∞—Ä—Ç–∏–Ω–∫–∞-—Ç–µ–∫—Å—Ç) –≤ —Å–≤–æ–π –æ—Ç–≤–µ—Ç **–î–û–°–õ–û–í–ù–û**, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n\n"
    "**–ü–†–ò–ú–ï–†:**\n"
    "–ö–û–ù–¢–ï–ö–°–¢: '...–≤ –ø–æ—è–≤–∏–≤—à–µ–º—Å—è –æ–∫–Ω–µ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É ¬´–î–∞–ª–µ–µ¬ª. ![–°–Ω–∏–º–æ–∫ –æ–∫–Ω–∞ —Å –∫–Ω–æ–ø–∫–æ–π –î–∞–ª–µ–µ](img1.png) –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –æ—Ç–∫—Ä—ã—Ç–∏—é —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞...'\n"
    "–ü–†–ê–í–ò–õ–¨–ù–´–ô –û–¢–í–ï–¢: '...–≤ –ø–æ—è–≤–∏–≤—à–µ–º—Å—è –æ–∫–Ω–µ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É ¬´–î–∞–ª–µ–µ¬ª. ![–°–Ω–∏–º–æ–∫ –æ–∫–Ω–∞ —Å –∫–Ω–æ–ø–∫–æ–π –î–∞–ª–µ–µ](img1.png) –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –æ—Ç–∫—Ä—ã—Ç–∏—é —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞...'\n"
    "–ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–ô –û–¢–í–ï–¢ (–ó–ê–ü–†–ï–©–ï–ù–û): '...–Ω–∞–∂–º–∏—Ç–µ ¬´–î–∞–ª–µ–µ¬ª, –∏ –æ—Ç–∫—Ä–æ–µ—Ç—Å—è –Ω–æ–≤—ã–π –¥–∏–∞–ª–æ–≥. ![–°–Ω–∏–º–æ–∫ –æ–∫–Ω–∞ —Å –∫–Ω–æ–ø–∫–æ–π –î–∞–ª–µ–µ](img1.png)'\n\n"
    "**–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê:**\n"
    "1. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –≤ –≤–æ–ø—Ä–æ—Å–µ –ø—Ä–æ–¥—É–∫—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–§–ü–°–£ –ê–º–∏–≥–æ'), –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¢–û–õ–¨–ö–û –ø–æ —ç—Ç–æ–º—É –ø—Ä–æ–¥—É–∫—Ç—É. –û—Å–Ω–æ–≤—ã–≤–∞–π –æ—Ç–≤–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –Ω–∏—Ö. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É –Ω–µ—Ç, —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.\n"
    "2. –ü–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–π —Å–µ–∫—Ü–∏—é `–ò—Å—Ç–æ—á–Ω–∏–∫–∏:`.\n"
    "3. –í –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: `–ü—Ä–æ–¥—É–∫—Ç: [–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞], –î–æ–∫—É–º–µ–Ω—Ç: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞]`."
)

agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_MSG),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)
agent = create_openai_functions_agent(llm, TOOLS, agent_prompt)
executor = AgentExecutor(agent=agent, tools=TOOLS, verbose=False, return_intermediate_steps=True)

def chat(user_input: str) -> dict:
    result = executor.invoke({"input": user_input})
    output_text = result.get("output", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.")
    tool_output = "\n".join(
        [str(step[1]) for step in result.get("intermediate_steps", [])]
    )
    return {"text": output_text, "tool_output": tool_output}

if __name__ == "__main__":
    while True:
        q = input("\nüë§ –í—ã: ")
        if q.lower().strip() in {"exit", "quit", "–≤—ã—Ö–æ–¥"}: break
        answer = chat(q)
        print("\nü™Ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è CLI:\n", answer['text'])