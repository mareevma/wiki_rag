from langchain_openai import ChatOpenAI
from langchain.tools import Tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from index_documents import get_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

"""Agentic RAG chatbot (ReAct-style) ---------------------------------------

This script demonstrates an "agentic" flavour of Retrieval-Augmented Generation.
Instead of running a fixed RAG chain, we expose the Vector Store retriever as an
explicit *tool*.  Using OpenAI function-calling, the language-model can decide,
step-by-step, when to call the retriever (possibly several times with different
queries) before formulating a final answer.

The overall design follows the Hugging Face "agent_rag" cookbook notebook
(https://huggingface.co/learn/cookbook/agent_rag) while re-using the existing
indexing/retrieval code that already ships with the project.
"""

# --------------------------------------------------------------------------------------
# 1. initialise LLM
# --------------------------------------------------------------------------------------

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# --------------------------------------------------------------------------------------
# 2. tool: semantic retriever
# --------------------------------------------------------------------------------------

# we keep using the ParentDocument retriever that proved to work well in chat_rag.py
retriever = get_retriever(use_parent_document=True)


def _search_knowledge_base(query: str, k: int = 7) -> str:  # noqa: D401
    """Retrieve *k* most relevant docs and return them as a printable string."""

    results = retriever.get_relevant_documents(query)[:k]
    if not results:
        return "No documents found."

    blob_parts = [
        f"===== Document {i} =====\n{doc.page_content}\n" for i, doc in enumerate(results)
    ]
    return "\n".join(blob_parts)


retriever_tool = Tool(
    name="retriever",
    func=_search_knowledge_base,
    description=(
        "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ **retriever** Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. "
        "ĞĞ° Ğ²Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ°Ñ‘Ñ‚ÑÑ **ÑƒÑ‚Ğ²ĞµÑ€Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ** (Ğ½Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ), ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰ĞµĞµ, Ñ‡Ñ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¹Ñ‚Ğ¸. "
        "Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‹Ñ€Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ. "
        "Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², "
        "Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚."
    ),
)

TOOLS = [retriever_tool]

# --------------------------------------------------------------------------------------
# 3. system prompt (+function agent)
# --------------------------------------------------------------------------------------

SYSTEM_MSG = (
    "Ğ’Ñ‹ â€” Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¤ĞŸĞ¡Ğ£-IP. "
    "ĞŸÑ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ (Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ), "
    "Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ `retriever` Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, "
    "Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ **Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ**. "
    "Ğ•ÑĞ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½ĞµÑ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ â€” Ñ‡ĞµÑÑ‚Ğ½Ğ¾ ÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑ‚Ğµ. "
    "Ğ’ĞĞ–ĞĞ: ĞµÑĞ»Ğ¸ Ğ² Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Markdown (![alt](file.png)), "
    "Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ¹Ñ‚Ğµ Ğ¸Ñ… Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚."
)

# Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ prompt Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° (OpenAI function calling)
agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_MSG),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
agent = create_openai_functions_agent(llm, TOOLS, agent_prompt)
executor = AgentExecutor(agent=agent, tools=TOOLS, verbose=True)

# --------------------------------------------------------------------------------------
# 4. CLI helper --------------------------------------------------------------
# --------------------------------------------------------------------------------------

def chat(user_input: str) -> str:
    """Run the Agentic RAG pipeline on *user_input* and return the model answer."""

    result = executor.invoke({"input": user_input})
    return result["output"]  # AgentExecutor stores agent answer under key "output"


if __name__ == "__main__":
    print("ğŸš€ Agentic RAG (function-calling) Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½! (Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ 'exit' Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°.)")
    try:
        while True:
            q = input("\nğŸ‘¤ Ğ’Ñ‹: ")
            if q.lower().strip() in {"exit", "quit", "Ğ²Ñ‹Ñ…Ğ¾Ğ´"}:
                print("ğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!")
                break
            if not q.strip():
                continue

            answer = chat(q)
            print("\nğŸª„ ĞÑ‚Ğ²ĞµÑ‚:\n", answer)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!") 