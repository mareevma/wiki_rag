import pathlib
import re
import unicodedata
import html
from itertools import chain
from typing import Optional

# --- –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ---
from charset_normalizer import from_path
from bs4 import BeautifulSoup, NavigableString
from markdownify import markdownify as mdify

# --- –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ LangChain ---
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_core.stores import BaseStore
import pickle
import os
from typing import Iterator, List, Tuple, Any
from langchain.schema import Document

# ==============================================================================
# 1. –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π docstore
# ==============================================================================

class PickleDocStore(BaseStore):
    """–ü—Ä–æ—Å—Ç–æ–µ —Ñ–∞–π–ª–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pickle"""
    
    def __init__(self, store_path="./docstore"):
        self.store_path = store_path
        os.makedirs(store_path, exist_ok=True)
        self.index_file = os.path.join(store_path, "index.pkl")
        self._load_index()
    
    def _load_index(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if os.path.exists(self.index_file):
            try:
                with open(self.index_file, 'rb') as f:
                    self._index = pickle.load(f)
            except:
                self._index = {}
        else:
            self._index = {}
    
    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        with open(self.index_file, 'wb') as f:
            pickle.dump(self._index, f)
    
    def mset(self, key_value_pairs: List[Tuple[str, Any]]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        for key, value in key_value_pairs:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∫–ª—é—á–∞
            safe_key = str(hash(key) % 10000000)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–º–µ–Ω–∏
            file_path = os.path.join(self.store_path, f"doc_{safe_key}.pkl")
            
            with open(file_path, 'wb') as f:
                pickle.dump(value, f)
            
            self._index[key] = file_path
        
        self._save_index()
    
    def mget(self, keys: List[str]) -> List[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        results = []
        for key in keys:
            if key in self._index:
                try:
                    with open(self._index[key], 'rb') as f:
                        results.append(pickle.load(f))
                except:
                    results.append(None)
            else:
                results.append(None)
        return results
    
    def mdelete(self, keys: List[str]) -> None:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        for key in keys:
            if key in self._index:
                try:
                    os.remove(self._index[key])
                except:
                    pass
                del self._index[key]
        self._save_index()
    
    def yield_keys(self, prefix: str = "") -> Iterator[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–∏"""
        for key in self._index.keys():
            if key.startswith(prefix):
                yield key

# ==============================================================================
# 2. –°–µ–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML (—Å —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π)
# ==============================================================================

def _clean(txt: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç: –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç, —É–±–∏—Ä–∞–µ—Ç –¥–≤–æ–π–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫."""
    txt = unicodedata.normalize("NFC", html.unescape(txt))
    return re.sub(r"\n{2,}", "\n", txt).strip()

def _html_to_md_with_imgs(soup: BeautifulSoup) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–µ–≥–∏ <img>."""
    for img in soup.find_all("img", src=True):
        img.replace_with(NavigableString(f"![{img.get('alt', '')}]({img['src']})"))
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º—É—Å–æ—Ä–∞
    for s in soup(['script', 'style']):
        s.decompose()
    return mdify(str(soup), heading_style="ATX")

def load_zoom_section_v3_optimized(path: pathlib.Path) -> Optional[dict]:
    """
    –ù–∞–¥—ë–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç HTML-—Ñ–∞–π–ª.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Ç–µ–≥–∞ <body>.
    """
    try:
        # 1) –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        results = from_path(path)
        best_match = results.best()
        raw_html = best_match.output(encoding=best_match.encoding) if best_match else path.read_text(encoding='utf-8', errors='replace')

        # 2) –ï–¥–∏–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(raw_html, "html.parser")

        # 3) –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_tag = soup.select_one("#printheader h1, #idheader h1, h1")
        title = title_tag.get_text(strip=True) if title_tag else (soup.title.get_text(strip=True) if soup.title else path.stem)

        # 4) –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ <body>
        body_tag = soup.find('body')
        if body_tag:
            content_soup = body_tag
        else:
            content_soup = None

        if not content_soup:
            return None # –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–≥ <body>

        # 5) –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown
        text_md = _clean(_html_to_md_with_imgs(content_soup))

        return {
            "title":   title,
            "text":    text_md,
            "source":  str(path.resolve()),
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {path}: {e}")
        return None

# ==============================================================================
# 2. –õ–æ–≥–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ==============================================================================

def iter_html_files(root: pathlib.Path):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .html —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–ø–∞–ø–∫–∞—Ö */HTML/ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–º—è_–ø—Ä–æ–¥—É–∫—Ç–∞, –ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É)."""
    for product_dir in root.iterdir():
        if not product_dir.is_dir():
            continue

        product_name = product_dir.name
        html_dir = product_dir / "HTML"
        if html_dir.exists() and html_dir.is_dir():
            yield from ((product_name, p) for p in html_dir.glob("*.html"))

def get_retriever(use_parent_document=True):  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π retriever –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ RAG —Å–∏—Å—Ç–µ–º–µ.
    
    Args:
        use_parent_document (bool): –ï—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ParentDocumentRetriever
                                   –ï—Å–ª–∏ False, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
    """
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", request_timeout=60)
    
    if use_parent_document:
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ParentDocumentRetriever...")
        
        # --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ParentDocumentRetriever —Å InMemoryStore ---
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,  # –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            chunk_overlap=50
        )
        
        vectorstore = Chroma(
            collection_name="docs_multiproduct_parent_v1",
            persist_directory="chroma_db",
            embedding_function=embeddings,
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π docstore
        docstore = PickleDocStore("./docstore")
        
        retriever = ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=docstore,
            child_splitter=child_splitter,
            parent_splitter=None,  # —Ö—Ä–∞–Ω–∏–º –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        )
        return retriever
    
    else:
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ retriever'–∞...")
        
        # --- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Å—Ç–æ–π retriever ---
        vectorstore = Chroma(
            collection_name="docs_multiproduct_v4_optimized",
            persist_directory="chroma_db",
            embedding_function=embeddings,
        )
        
        # –ë–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 8}  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 6 –¥–æ 8
        )
        return retriever

def main(use_parent_document=True, max_files=None):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤—Å–µ—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.
    
    Args:
        use_parent_document (bool): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ParentDocumentRetriever
        max_files (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –≤—Å–µ)
    """
    mode_name = "ParentDocumentRetriever" if use_parent_document else "–ø—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫"
    if max_files:
        print(f"üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({mode_name}) - –¢–ï–°–¢ —Ä–µ–∂–∏–º: {max_files} —Ñ–∞–π–ª–æ–≤...")
    else:
        print(f"üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({mode_name})...")
    ROOT = pathlib.Path("data")

    # --- –®–∞–≥ 1: –ß—Ç–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ HTML-—Ñ–∞–π–ª–æ–≤ ---
    parent_docs = []
    seen_paths = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    processed_count = 0

    for product, path in iter_html_files(ROOT):
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
        if max_files and processed_count >= max_files:
            print(f"üîÑ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤: {max_files}")
            break
            
        path_str = str(path.resolve())
        if path_str in seen_paths:
            continue
        seen_paths.add(path_str)

        parsed_content = load_zoom_section_v3_optimized(path)
        if not parsed_content or not parsed_content["text"].strip(): # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            print(f"‚è© –ü—Ä–æ–ø—É—â–µ–Ω –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–∞–π–ª: {path.name}")
            continue
        
        doc = Document(
            page_content=parsed_content["text"],
            metadata={
                "title": parsed_content["title"],
                "source": parsed_content["source"],
                "product": product,
                "file_path": path_str  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø—É—Ç—å –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            }
        )
        parent_docs.append(doc)
        processed_count += 1
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
        if processed_count % 50 == 0:
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_count}")

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(parent_docs)}")
    if max_files:
        print(f"üéØ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –∏–∑ {max_files} –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")

    if not parent_docs:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–∫–∏ 'data/'.")
        return

    # --- –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ retriever'–∞ ---
    retriever = get_retriever(use_parent_document=use_parent_document)

    if use_parent_document:
        # --- ParentDocumentRetriever: –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–ø—Ä—è–º—É—é ---
        print("‚è≥ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ParentDocumentRetriever...")
        
        batch_size = 100  # –º–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è ParentDocument (–±–æ–ª—å—à–µ –æ–ø–µ—Ä–∞—Ü–∏–π)
        total_docs = len(parent_docs)
        
        for i in range(0, total_docs, batch_size):
            batch_docs = parent_docs[i:i + batch_size]
            
            print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(total_docs + batch_size - 1)//batch_size} ({len(batch_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            
            try:
                # –ü–µ—Ä–µ–¥–∞–µ–º ids=None —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ LangChain
                retriever.add_documents(batch_docs, ids=None)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {i//batch_size + 1}: {e}")
                # –ü—Ä–æ–±—É–µ–º –º–µ–Ω—å—à–∏–π –±–∞—Ç—á
                smaller_batch_size = max(1, batch_size // 4)
                print(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞: {smaller_batch_size}")
                for j in range(i, min(i + batch_size, total_docs), smaller_batch_size):
                    small_batch_docs = parent_docs[j:j + smaller_batch_size]
                    try:
                        retriever.add_documents(small_batch_docs, ids=None)
                    except Exception as inner_e:
                        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã: {inner_e}")
                        
    else:
        # --- –ü—Ä–æ—Å—Ç–æ–π retriever: —Å–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ ---
        print("üìÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
            chunk_overlap=80
        )
        
        all_splits = []
        for doc in parent_docs:
            splits = text_splitter.split_documents([doc])
            all_splits.extend(splits)
        
        print(f"‚úÇÔ∏è  –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(all_splits)}")
        
        print("‚è≥ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
        vectorstore = retriever.vectorstore
        
        batch_size = 200  # –±–æ–ª—å—à–∏–π –±–∞—Ç—á –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —á–∞–Ω–∫–æ–≤
        total_docs = len(all_splits)
        
        for i in range(0, total_docs, batch_size):
            batch_docs = all_splits[i:i + batch_size]
            
            print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(total_docs + batch_size - 1)//batch_size} ({len(batch_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            
            try:
                vectorstore.add_documents(batch_docs)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {i//batch_size + 1}: {e}")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –º–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                smaller_batch_size = max(1, batch_size // 2)
                print(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞: {smaller_batch_size}")
                for j in range(i, min(i + batch_size, total_docs), smaller_batch_size):
                    small_batch_docs = all_splits[j:j + smaller_batch_size]
                    try:
                        vectorstore.add_documents(small_batch_docs)
                    except Exception as inner_e:
                        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã: {inner_e}")
    
    print("üîç –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    import sys
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    use_parent_document = "--simple" not in sys.argv  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ParentDocument
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    max_files = None
    if "--test" in sys.argv:
        max_files = 100  # –¢–µ—Å—Ç —Ä–µ–∂–∏–º: —Ç–æ–ª—å–∫–æ 100 —Ñ–∞–π–ª–æ–≤
    elif "--small-test" in sys.argv:
        max_files = 20   # –ú–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç: —Ç–æ–ª—å–∫–æ 20 —Ñ–∞–π–ª–æ–≤
    
    if use_parent_document:
        print("üéØ –†–µ–∂–∏–º: ParentDocumentRetriever (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
        print("üí° –î–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python index_documents.py --simple")
    else:
        print("üéØ –†–µ–∂–∏–º: –ü—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫") 
        print("üí° –î–ª—è ParentDocument —Ä–µ–∂–∏–º–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python index_documents.py")
    
    if max_files:
        print(f"üß™ –¢–µ—Å—Ç —Ä–µ–∂–∏–º: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {max_files} —Ñ–∞–π–ª–æ–≤")
        print("üí° –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã:")
        print("   --test: 100 —Ñ–∞–π–ª–æ–≤")
        print("   --small-test: 20 —Ñ–∞–π–ª–æ–≤")
        print("   (–±–µ–∑ —Ñ–ª–∞–≥–æ–≤): –≤—Å–µ —Ñ–∞–π–ª—ã")
    
    main(use_parent_document=use_parent_document, max_files=max_files)