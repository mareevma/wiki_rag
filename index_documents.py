# index_documents.py (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)

import pathlib
import re
import unicodedata
import html
import pickle
import multiprocessing
import os
from typing import List, Optional

import tiktoken
from bs4 import BeautifulSoup
from charset_normalizer import from_path
from dotenv import load_dotenv
from langchain.retrievers import ParentDocumentRetriever
from langchain.schema import Document
from langchain.storage import EncoderBackedStore
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.storage import SQLStore
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from markdownify import markdownify as mdify

load_dotenv()

# --- –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∏ –±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å–µ–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ---

def _clean(txt: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –∏ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π."""
    txt = unicodedata.normalize("NFC", html.unescape(txt))
    return re.sub(r"\n{2,}", "\n", txt).strip()

def _html_to_md(soup: BeautifulSoup) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown. 
    markdownify —Å–∞–º–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç <img>, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏.
    """
    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏ –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π
    for s in soup(['script', 'style']):
        s.decompose()
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º body –≤ Markdown, heading_style="ATX" –¥–µ–ª–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–∏–¥–∞ #, ##
    return mdify(str(soup), heading_style="ATX")

def parse_file(path: pathlib.Path) -> Optional[dict]:
    """
    –ü–∞—Ä—Å–∏—Ç HTML-—Ñ–∞–π–ª, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    –í–∞–∂–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Å–∞–º–æ–º—É HTML-—Ñ–∞–π–ª—É –≤ `source`.
    """
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        results = from_path(path)
        best_match = results.best()
        raw_html = best_match.output(encoding=best_match.encoding) if best_match else path.read_text(encoding='utf-8', errors='replace')
        
        soup = BeautifulSoup(raw_html, "html.parser")
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_tag = soup.select_one("#printheader h1, #idheader h1, h1")
        title = title_tag.get_text(strip=True) if title_tag else (soup.title.get_text(strip=True) if soup.title else path.stem)
        
        body_tag = soup.find('body')
        if not body_tag: return None
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π Markdown —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º
        text_md = _clean(_html_to_md(body_tag))
        
        if not text_md.strip(): return None

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏. `source` —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        # –≠—Ç–æ –ö–õ–Æ–ß–ï–í–û–ô –º–æ–º–µ–Ω—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–æ–∫.
        return {
            "title": title,
            "text": text_md,
            "source": str(path.resolve()), # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ HTML —Ñ–∞–π–ª—É
            "product": path.parent.parent.name,
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {path}: {e}")
        return None

# ==============================================================================
# –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ñ–∞–π–ª–∞ `index_documents.py` –æ—Å—Ç–∞–µ—Ç—Å—è –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô.
# –ü—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ –µ–µ —Å—é–¥–∞ –∏–∑ –≤–∞—à–µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
# ... (iter_html_files, get_retriever, count_tokens_for_documents, main, etc.)
# ==============================================================================

# (–Ω–∞—á–∞–ª–æ –Ω–µ–∏–∑–º–µ–Ω–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏)
def iter_html_files(root: pathlib.Path):
    html_files = list(root.glob("**/HTML/*.html"))
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    return html_files

def get_retriever(use_parent_document=True, use_openai_embeddings=True):
    if use_openai_embeddings:
        print("ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ OpenAI (text-embedding-3-small)...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", request_timeout=120)
    else:
        print("ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Hugging Face)...")
        model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )

    if use_parent_document:
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ParentDocumentRetriever...")
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
        vectorstore = Chroma(collection_name="docs_multiproduct_parent_v3_relative", persist_directory="chroma_db", embedding_function=embeddings)
        
        underlying_store = SQLStore(namespace="documents_v3", db_url="sqlite:///docstore.db")
        underlying_store.create_schema()
        store = EncoderBackedStore(
            store=underlying_store,
            key_encoder=lambda x: x,
            value_serializer=pickle.dumps,
            value_deserializer=pickle.loads
        )

        retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)
        return retriever
    else:
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ retriever'–∞...")
        vectorstore = Chroma(collection_name="docs_multiproduct_v5_optimized", persist_directory="chroma_db", embedding_function=embeddings)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 8})
        return retriever

def count_tokens_for_documents(documents: List[Document], model_name: str = "cl100k_base") -> int:
    try:
        encoding = tiktoken.get_encoding(model_name)
    except ValueError:
        encoding = tiktoken.get_encoding("gpt2")
    total_tokens = sum(len(encoding.encode(doc.page_content)) for doc in documents)
    return total_tokens

def main(use_parent_document=True, max_files=None, use_openai=True):
    mode_name = "ParentDocumentRetriever" if use_parent_document else "–ø—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫"
    print(f"üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({mode_name})...")
    ROOT = pathlib.Path("data")

    files_to_process = iter_html_files(ROOT)
    if max_files:
        files_to_process = files_to_process[:max_files]
        print(f"üîÑ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤—ã—Ö {max_files} —Ñ–∞–π–ª–æ–≤.")

    parent_docs = []
    with multiprocessing.Pool() as pool:
        results = pool.map(parse_file, files_to_process)

    for parsed_content in results:
        if parsed_content:
            parent_docs.append(Document(page_content=parsed_content["text"], metadata={
                "title": parsed_content["title"], 
                "source": parsed_content["source"], # <-- –í–∞–∂–Ω–æ, —á—Ç–æ –∑–¥–µ—Å—å –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
                "product": parsed_content["product"],
            }))

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(parent_docs)}")
    if not parent_docs:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return

    if use_openai:
        total_tokens = count_tokens_for_documents(parent_docs)
        price_per_million_tokens = 0.02
        estimated_cost = (total_tokens / 1_000_000) * price_per_million_tokens
        print("\n" + "="*50)
        print("üìä –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (OpenAI)")
        print(f"   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {total_tokens:,}")
        print(f"   –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${estimated_cost:.6f}")
        print("="*50 + "\n")
    else:
        print("\n" + "="*50 + "\nüìä –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏." + "\n" + "="*50 + "\n")

    retriever = get_retriever(use_parent_document=use_parent_document, use_openai_embeddings=use_openai)

    batch_size = 100
    total_docs = len(parent_docs)
    
    print("‚è≥ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ retriever –ø–æ—Ä—Ü–∏—è–º–∏...")
    for i in range(0, total_docs, batch_size):
        batch_docs = parent_docs[i:i + batch_size]
        print(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(total_docs + batch_size - 1)//batch_size} ({len(batch_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
        
        retriever.add_documents(batch_docs, ids=None)

    print("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    import sys
    # –í–∞–∂–Ω–æ: –ü–æ—Å–ª–µ —Å–º–µ–Ω—ã –ª–æ–≥–∏–∫–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
    # –ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Chroma, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
    print("‚ÄºÔ∏è –í–ê–ñ–ù–û: –õ–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—É—é –±–∞–∑—É Chroma (–ø–∞–ø–∫—É chroma_db) –¥–ª—è –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
    
    use_parent_document = "--simple" not in sys.argv
    use_openai_emb = "--local-embeddings" not in sys.argv
    max_files = None
    if "--test" in sys.argv: max_files = 100
    
    print(f"üéØ –†–µ–∂–∏–º: {'ParentDocumentRetriever' if use_parent_document else '–ü—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π'}")
    print(f"ü§ñ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {'OpenAI' if use_openai_emb else '–õ–æ–∫–∞–ª—å–Ω—ã–µ (Hugging Face)'}")
    if use_openai_emb and not os.getenv("OPENAI_API_KEY"):
        print("\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –Ω–∞–π–¥–µ–Ω OPENAI_API_KEY. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω –µ—Å—Ç—å –≤ .env —Ñ–∞–π–ª–µ.")
    
    main(use_parent_document=use_parent_document, max_files=max_files, use_openai=use_openai_emb)